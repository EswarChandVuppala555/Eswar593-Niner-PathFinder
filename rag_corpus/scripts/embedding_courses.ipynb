{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Vector Embeddings - Sub-File Level (Course Descriptions Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "def course_description_processing(\n",
    "        input_file_path: str,\n",
    "        output_file_path: str,\n",
    "        delimiter: str,\n",
    "        client,\n",
    "        embedding_model_name):\n",
    "\n",
    "    with open(input_file_path, 'r', encoding='utf8', errors = 'ignore') as f:\n",
    "        file_contents = f.read()\n",
    "\n",
    "    print('File opened successfully.')\n",
    "\n",
    "    # Split content by delimiter\n",
    "    chunks = file_contents.split(delimiter)\n",
    "\n",
    "    print(f\"Split content into {len(chunks)} chunks.\")\n",
    "\n",
    "    course_data = []\n",
    "\n",
    "    for i in range(len(chunks)):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing chunk {i}...\")\n",
    "\n",
    "        chunk_content = chunks[i].strip().replace('\\n', ' ')\n",
    "\n",
    "        if len(chunk_content) < 20:\n",
    "            print(f\"Skipping chunk {i} with length {len(chunk_content)} and content '{chunk_content}' due to inadequate length\\n\\n\")\n",
    "            continue\n",
    "\n",
    "        # Get the name of the file from course department and number in characters 3-11 (DEPT ####):\n",
    "        chunk_name = chunk_content[2:12].replace('.', '')\n",
    "\n",
    "        # Identify invalid chunk names \n",
    "        if not re.match(r'^[A-Z]{4} \\d{4}L?$', chunk_name):\n",
    "            print(f\"Skipping chunk {i} due to invalid name '{chunk_name} with content '{chunk_content}' '\\n\\n\")\n",
    "            continue      \n",
    "\n",
    "        # Get embedding from OpenAI\n",
    "        embedding = client.embeddings.create(\n",
    "            input=chunk_content,\n",
    "            model=embedding_model_name\n",
    "        )\n",
    "        \n",
    "        # Store course data with name, content, and embedding\n",
    "        course_entry = {\n",
    "            'name': chunk_name,\n",
    "            'content': chunk_content,\n",
    "            'embedding': embedding.data[0].embedding\n",
    "        }\n",
    "\n",
    "        if i % 10 ==0:\n",
    "            print(course_entry)\n",
    "\n",
    "            \n",
    "        course_data.append(course_entry)\n",
    "    \n",
    "    # identify duplicate course names\n",
    "    course_names = [entry['name'] for entry in course_data]\n",
    "    duplicates = set([name for name in course_names if course_names.count(name) > 1])\n",
    "    if duplicates:\n",
    "        print(f\"Found duplicate course names: {duplicates}\")\n",
    "    else:\n",
    "        print(\"No duplicate course names found.\")\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_file_path, \"w\") as f:\n",
    "        json.dump(course_data, f, indent=2)\n",
    "        \n",
    "    print(f\"Saved {len(course_data)} course entries to {output_file_path}\")\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model_name = os.getenv(\"EMBEDDING_MODEL_NAME\")\n",
    "client = OpenAI()\n",
    "\n",
    "input_file_path = 'rag_corpus/ug_cat/staged/courses/course_descriptions.md'\n",
    "output_file_path = 'rag_corpus/ug_cat/staged/courses/course_descriptions_with_embeddings_new.json'\n",
    "\n",
    "course_description_processing(\n",
    "    input_file_path=input_file_path,\n",
    "    output_file_path=output_file_path,\n",
    "    delimiter='\\n\\n',\n",
    "    client=client,\n",
    "    embedding_model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from the JSON file\n",
    "with open(output_file_path, 'r') as f:\n",
    "    course_descriptions_with_embeddings = json.load(f)\n",
    "\n",
    "    for k,v in course_descriptions_with_embeddings.items():\n",
    "        print(f\"Key: {k}, \\nDescription: {v[0][:50]}..., \\nEmbedding: {v[1][:5]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# Split the course descriptions file into smaller files\n",
    "def split_course_descriptions(input_file_path, output_dir, chunk_size=100):\n",
    "    with open(input_file_path, 'r') as f:\n",
    "        course_descriptions = json.load(f)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for i in range(0, len(course_descriptions), chunk_size):\n",
    "        chunk = course_descriptions[i:i + chunk_size]\n",
    "        output_file_path = os.path.join(output_dir, f'course_descriptions_chunk_{i // chunk_size + 1}.json')\n",
    "        \n",
    "        with open(output_file_path, 'w') as out_f:\n",
    "            json.dump(chunk, out_f, indent=2)\n",
    "        \n",
    "        print(f\"Saved chunk {i // chunk_size + 1} with {len(chunk)} entries to {output_file_path}\")\n",
    "\n",
    "output_file_path = 'rag_corpus/ug_cat/staged/courses/course_descriptions_with_embeddings.json'\n",
    "\n",
    "# Split the course descriptions into smaller files\n",
    "output_dir = 'rag_corpus/ug_cat/staged/courses/'\n",
    "split_course_descriptions(\n",
    "    input_file_path=output_file_path,\n",
    "    output_dir=output_dir,\n",
    "    chunk_size=500)  # Adjust chunk size as needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
